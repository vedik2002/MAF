{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WELDV3kKvsng"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
        "\n",
        "from transformers.modeling_utils import PreTrainedModel, unwrap_model\n",
        "\n",
        "from transformers import (\n",
        "    MBart50TokenizerFast,\n",
        "    AdamW\n",
        ")\n",
        "\n",
        "from transformers.models.mbart.configuration_mbart import MBartConfig\n",
        "\n",
        "from transformers.models.mbart.modeling_mbart import (\n",
        "    MBartPreTrainedModel,\n",
        "    MBartDecoder,\n",
        "    MBartLearnedPositionalEmbedding,\n",
        "    MBartEncoderLayer,\n",
        "    shift_tokens_right,\n",
        ")\n",
        "\n",
        "\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutput,\n",
        "    Seq2SeqLMOutput,\n",
        "    Seq2SeqModelOutput\n",
        ")\n",
        "\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalMBartEncoder(MBartPreTrainedModel):\n",
        "\n",
        "  def __init__(self, config: MBartConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.encoder_layerdrop\n",
        "\n",
        "        embed_dim = config.d_model\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.max_source_positions = config.max_position_embeddings\n",
        "        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
        "\n",
        "        if embed_tokens is not None:\n",
        "            self.embed_tokens = embed_tokens\n",
        "        else:\n",
        "            self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n",
        "\n",
        "        self.embed_positions = MBartLearnedPositionalEmbedding(\n",
        "            config.max_position_embeddings,\n",
        "            embed_dim,\n",
        "        )\n",
        "        self.layers = nn.ModuleList([MBartEncoderLayer(config) for _ in range(config.encoder_layers)])\n",
        "        self.layernorm_embedding = nn.LayerNorm(embed_dim)\n",
        "        self.layer_norm = nn.LayerNorm(config.d_model)\n",
        "\n",
        "        self.init_weights()\n",
        "        self.gradient_checkpointing = False\n",
        "\n",
        "  def forward(self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        #acoustic_input=None,      # New addition of acoustic_input\n",
        "        #visual_input=None,      # New addition of visual_input\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,)-> Union[Tuple, BaseModelOutput]:\n",
        "\n",
        "\n",
        "\n",
        "      #Handeling empty inputs\n",
        "      '''\n",
        "      output_attentions (`bool`, *optional*):Whether or not to return the attentions tensors of all attention layers.\n",
        "      output_hidden_states (`bool`, *optional*):Whether or not to return the hidden states of all layers\n",
        "      return_dict (`bool`, *optional*): Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
        "      '''\n",
        "\n",
        "      output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "      output_hidden_states =(\n",
        "              output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "          )\n",
        "      return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "\n",
        "      '''\n",
        "      Checking for input ids and input embeds if input ids is given then we are forming embedinng from it but if\n",
        "      embediing if given we are extracting input_ids from it dierectly\n",
        "      '''\n",
        "\n",
        "      if input_ids is not None and inputs_embeds is not None:\n",
        "              raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "      elif input_ids is not None:\n",
        "              input = input_ids\n",
        "              input_shape = input.shape\n",
        "              input_ids = input_ids.view(-1, input_shape[-1])\n",
        "      elif inputs_embeds is not None:\n",
        "              input = inputs_embeds[:, :, -1]\n",
        "      else:\n",
        "          raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "      if inputs_embeds is None:\n",
        "        inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
        "\n",
        "\n",
        "\n",
        "      embed_pos = self.embed_positions(input) #Getting the positional embeddings\n",
        "      hidden_states = inputs_embeds + embed_pos.to(inputs_embeds.device) #Adding word and positional embeddings\n",
        "      hidden_states = self.layernorm_embedding(hidden_states) #for preventing covariance shift\n",
        "      hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training) #why\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tXMqTklb389G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalMBartModel(MBartPreTrainedModel):\n",
        "\n",
        "  def __init__(self, config: MBartConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.encoder = MultimodalMBartEncoder(config, self.shared)\n",
        "\n",
        "  def get_encoder(self):\n",
        "      return self.encoder\n",
        "\n"
      ],
      "metadata": {
        "id": "U5t9quK8xHwy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sv79IQF-0u4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalMBartForConditionalGeneration(MBartPreTrainedModel):\n",
        "\n",
        "  def __init__(self, config: MBartConfig):\n",
        "\n",
        "        super().__init__(config)\n",
        "        self.model = MultimodalMBartModel(config)\n",
        "        #self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
        "        #self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n",
        "\n",
        "        self.init_weights()\n",
        ""
      ],
      "metadata": {
        "id": "wjegr0hDwcPj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "MODEL = MultimodalMBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50-many-to-many-mmt')\n",
        "\n",
        "print(\"Model loaded...\\n\")\n",
        "MODEL.to(DEVICE)\n",
        "TOKENIZER = MBart50TokenizerFast.from_pretrained('facebook/mbart-large-50-many-to-many-mmt',\n",
        "                                                     src_lang=\"en_XX\",\n",
        "                                                     tgt_lang=\"en_XX\")\n",
        "print(\"Tokenizer loaded...\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9qlTa6Mv9bK",
        "outputId": "21857c20-0d65-4d0c-a8a9-5c6cfd042eae"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded...\n",
            "\n",
            "Tokenizer loaded...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ur-kJ9Iewfno"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}